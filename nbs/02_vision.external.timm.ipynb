{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp vision.timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing the `timm` Library Inside of `fastai` (Intermediate)\n",
    "\n",
    "> How to bring the power of Transfer Learning with new architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from wwf.utils import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "---\n",
       "This article is also a Jupyter Notebook available to be run from the top down. There\n",
       "will be code snippets that you can then run in any environment.\n",
       "\n",
       "Below are the versions of `fastai`, `fastcore`, and `timm` currently running at the time of writing this:\n",
       "* `fastai` : 2.3.0 \n",
       "* `fastcore` : 1.3.19 \n",
       "* `timm` : 0.4.5 \n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "state_versions(['fastai', 'fastcore', 'timm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing in External Models into the Framework\n",
    "\n",
    "As we are well aware, `fastai` models deep down are just `PyTorch` models. However as the field of Machine Learning keeps going, new and fresh architectures are introduced. Wouldn't it be nice if it were easy to integrate them into the `fastai` framework and play with them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ross Wightman's `timm` Library\n",
    "\n",
    "[Ross Wightman](https://twitter.com/wightmanr) has been on a mission to get pretrained weights for the newest Computer Vision models that come out of papers, and compare his results what the papers state themselves. The fantastic results live in his repository [here](https://github.com/rwightman/pytorch-image-models)\n",
    "\n",
    "For users of the `fastai` library, it is a goldmine of models to play with! But how do we use it? Let's set up a basic `PETs` problem following the [tutorial](https://walkwithfastai.com/vision.clas.single_label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.PETS)\n",
    "pat = r'/([^/]+)_\\d+.*'\n",
    "item_tfms = RandomResizedCrop(460, min_scale=0.75, ratio=(1.,1.))\n",
    "batch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]\n",
    "bs=16\n",
    "pets = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(0.2),\n",
    "                 get_y=RegexLabeller(pat = pat),\n",
    "                 item_tfms=item_tfms,\n",
    "                 batch_tfms=batch_tfms)\n",
    "dls = pets.dataloaders(path/'images', bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we would normally do something like `cnn_learner(dls, arch, metrics)`, however we need to do a few things special to work with Ross' framework.\n",
    "\n",
    "`fastai` has a `create_body` function, which is called during `cnn_learner`, that will take a model architecuture and slice off the last Linear layer (resulting in a \"body\" that outputs unpooled features). This function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_body(arch, n_in=3, pretrained=True, cut=None):\n",
    "    \"Cut off the body of a typically pretrained `arch` as determined by `cut`\"\n",
    "    model = arch(pretrained=pretrained)\n",
    "    _update_first_layer(model, n_in, pretrained)\n",
    "    if cut is None:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "    if   isinstance(cut, int):      return nn.Sequential(*list(model.children())[:cut])\n",
    "    elif callable(cut): return cut(model)\n",
    "    else:                           raise NamedError(\"cut must be either integer or a function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create our own that plays well\n",
    "> Also: notebooks like this are exported as external modules inside of the `wwf` library! This one can be found in `vision.timm` to be used with your projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from timm import create_model\n",
    "from fastai.vision.learner import _update_first_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):\n",
    "    \"Creates a body from any model in the `timm` library.\"\n",
    "    model = create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')\n",
    "    _update_first_layer(model, n_in, pretrained)\n",
    "    if cut is None:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "    if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n",
    "    elif callable(cut): return cut(model)\n",
    "    else: raise NamedError(\"cut must be either integer or function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we use it? Let's try it out on an `efficientnet_b3` architecture (the entire list of supported architectures is found [here](https://github.com/rwightman/pytorch-image-models#models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = create_timm_body('efficientnet_b3a', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can calculate the number input features our head needs to have with `num_features_model`.  We can pass `concat_pool=True` to have fastai create a head with two pooling layers: `AdaptiveConcatPool2d` and `nn.AdaptiveAvgPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf = num_features_model(body); nf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can create a head!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = create_head(nf, dls.c, concat_pool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mix them together, we just wrap the two in a `nn.Sequential` and we now have a `PyTorch` model ready to be trained on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(body, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we would pass it onto `Learner`, specifying our `splitter` to be the `default_split`\n",
    "> `default_splitter` expects the body in `model[0]` and the head in `model[1]` to split our layer groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, net, splitter=default_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know this all worked properly, we should be able to call `learn.freeze()` and check the number of frozen parameters. (You can also call `learn.summary` but we are not since it has a lengthy output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()\n",
    "frozen = filter(lambda p: not p.requires_grad, learn.model.parameters())\n",
    "frozen = sum([np.prod(p.size()) for p in unfrozen_params])\n",
    "model_parameters = filter(lambda p: p.requires_grad, learn.model.parameters())\n",
    "unfrozen = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10608936, 1686272)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frozen, unfrozen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can see that only 1.6 million of the 10 million parameters are trainable, so our model is ready for transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning it all into a function\n",
    "\n",
    "Let's make this a bit easier and create something like `cnn_learner`, but for `timm`! We'll call it a `timm_learner`. First let's look at and compare what `cnn_learner` does internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_learner(dls, arch, loss_func=None, pretrained=True, cut=None, splitter=None,\n",
    "                y_range=None, config=None, n_out=None, normalize=True, **kwargs):\n",
    "    \"Build a convnet style learner from `dls` and `arch`\"\n",
    "    if config is None: config = {}\n",
    "    meta = model_meta.get(arch, _default_meta)\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    if normalize: _add_norm(dls, meta, pretrained)\n",
    "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
    "    model = create_cnn_model(arch, n_out, ifnone(cut, meta['cut']), pretrained, y_range=y_range, **config)\n",
    "    learn = Learner(dls, model, loss_func=loss_func, splitter=ifnone(splitter, meta['split']), **kwargs)\n",
    "    if pretrained: learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first it looks scary, but let's try and read it as best we can:\n",
    "1. Grab potential private meta about an architecture we're using\n",
    "2. Grab the number of expected outputs\n",
    "3. Potentially normalize\n",
    "4. Add a `y_range`\n",
    "5. Create a `cnn_model` and `Learner`\n",
    "6. Freeze our model\n",
    "\n",
    "We're going to make a custom `create_timm_model` and `timm_learner` function to do what we just did above. First, `create_timm_model` will model after `create_cnn_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def create_timm_model(arch:str, n_out, cut=None, pretrained=True, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n",
    "                     concat_pool=True, **kwargs):\n",
    "    \"Create custom architecture using `arch`, `n_in` and `n_out` from the `timm` library\"\n",
    "    body = create_timm_body(arch, pretrained, None, n_in)\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(nn.Sequential(*body.children()))\n",
    "        head = create_head(nf, n_out, concat_pool=concat_pool, **kwargs)\n",
    "    else: head = custom_head\n",
    "    model = nn.Sequential(body, head)\n",
    "    if init is not None: apply_init(model[1], init)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for our `timm_learner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.learner import _add_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def timm_learner(dls, arch:str, loss_func=None, pretrained=True, cut=None, splitter=None,\n",
    "                y_range=None, config=None, n_out=None, normalize=True, **kwargs):\n",
    "    \"Build a convnet style learner from `dls` and `arch` using the `timm` library\"\n",
    "    if config is None: config = {}\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
    "    model = create_timm_model(arch, n_out, default_split, pretrained, y_range=y_range, **config)\n",
    "    learn = Learner(dls, model, loss_func=loss_func, splitter=default_split, **kwargs)\n",
    "    if pretrained: learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out by making the same model we did a moment ago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = timm_learner(dls, 'efficientnet_b3a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to verify let's look at those parameters one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen = filter(lambda p: not p.requires_grad, learn.model.parameters())\n",
    "frozen = sum([np.prod(p.size()) for p in unfrozen_params])\n",
    "model_parameters = filter(lambda p: p.requires_grad, learn.model.parameters())\n",
    "unfrozen = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1686272, 10608936)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfrozen_params, frozen_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're exactly the same! So now we can utilize any architecture found inside of `timm` right away, and we built it in a structure very similar to how native `fastai` does it. \n",
    "\n",
    "To use this module in your own work, simply do:\n",
    "```python\n",
    "from wwf.vision.timm import *\n",
    "learn = timm_learner(dls, 'efficientnet_b3a', metrics=[error_rate, accuracy])\n",
    "```\n",
    "\n",
    "> Note: `timm` needs to be installed beforehand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Lookup\n",
    "\n",
    "To query various models to see what is available, you should directly use the `timm` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all models available\n",
    "\n",
    "One option is to list every model possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adv_inception_v3',\n",
       " 'cspdarknet53',\n",
       " 'cspdarknet53_iabn',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'cspresnext50_iabn',\n",
       " 'darknet53',\n",
       " 'densenet121']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for models\n",
    "\n",
    "You can also query the names of what is available as well, denoted as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b2a',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b3a',\n",
       " 'efficientnet_b4']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*efficientnet*')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficientnet_b3a']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('*b3a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet18',\n",
       " 'resnet26']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('resne*t*', pretrained=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Warnings\n",
    "\n",
    "* Watch for anything with a `tf_` prefix. This means the original weights were ported from Google, so it uses manual padding to match TensorFlow's \"same\" padding, which adds GPU overhead and a general slowdown. If possible try to use the non-TF versions of models\n",
    "\n",
    "* HRNet is a bit of a problem-child, so it is the only one not straight-forward to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unet integration\n",
    "\n",
    "Many of the networks included in `timm` enjoy a second life as \"backbones\" for Unets (https://arxiv.org/abs/1505.04597), a popular architecture for segmentation (\"is there a pedestrian at this pixel?\") and similar vision tasks whose outputs have the same height and width as their corresponding inputs. A Unet consists of an encoder – structurally very similar to a headless classifier – and a decoder whose main role is to upscale and refine the encoder output. But there is a twist: the decoder also looks at features computed partway along the encoder. So we first need to get the encoder to output a *sequence* of features computed at various scales. Fortunately `timm` can do that for us: we only need to call `create_model` with the optional argument `features_only=True`. The rest is a bit more work. \n",
    "\n",
    "We roughly follow the structure and settings of `DynamicUnet` in `fastai`, also taking inspiration from Ross Wightman's prototype Unet code (https://gist.github.com/rwightman/f8b24f4e6f5504aba03e999e02460d31).\n",
    "\n",
    "The encoder is fully provided by `timm`, so the next step is to build a decoder which can accept multiple inputs at various scales. We will build it from standardised blocks. It is standard practice to imbue the Unet with a \"bottleneck\" layer at the bottom of the \"U\", and we will integrate it into the decoder itself. You can pass it a string (`'conv'`, `'attention'` or `'double_attention'`), a `Module` or nothing and it'll work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, up_in_c, s_in_c, scale=2, blur=False,  final_div=True,\n",
    "                 act_cls=defaults.activation, init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, scale=scale, blur=blur, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.bn = BatchNorm(s_in_c)\n",
    "        self.act = act_cls()\n",
    "        ni = up_in_c//2 + s_in_ct\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.nf = nf\n",
    "        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        apply_init(nn.Sequential(self.shuf, self.bn, self.conv1, self.conv2), init)\n",
    "\n",
    "    def forward(self, up_in: torch.Tensor, skip: Optional[torch.Tensor] = None):\n",
    "        x = self.shuf(up_in)\n",
    "        if skip is not None:\n",
    "            ssh = skip.shape[-2:]\n",
    "            if ssh != x.shape[-2:]:\n",
    "                x = F.interpolate(x, ssh, mode='nearest')\n",
    "            x = self.act(torch.cat([x, self.bn(skip)], dim=1))\n",
    "        return self.conv2(self.conv1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@delegates(nn.BatchNorm2d)\n",
    "def BatchNormZero(nf, ndim=2, **kwargs):\n",
    "    \"BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`. Weights initialized to zero.\"\n",
    "    return _get_norm('BatchNorm', nf, ndim, zero=True, **kwargs)\n",
    "\n",
    "\n",
    "def _make_bottleneck(bottleneck, ni, act_cls=defaults.activation, norm_type=None, init=nn.init.kaiming_normal_, **kwargs):\n",
    "    if bottleneck == 'conv':\n",
    "        seq = nn.Sequential(BatchNorm(ni), nn.ReLU(),\n",
    "                            ConvLayer(ni, ni*2, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "                            ConvLayer(ni*2, ni, act_cls=act_cls, norm_type=norm_type, **kwargs))\n",
    "    elif bottleneck == 'attention':\n",
    "        seq = nn.Sequential(BatchNormZero(ni), nn.ReLU(),\n",
    "                            ConvLayer(ni, ni, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "                            SimpleSelfAttention(ni, ks=1))\n",
    "    elif bottleneck == 'double_attention':\n",
    "        seq = nn.Sequential(BatchNormZero(ni), nn.ReLU(),\n",
    "                            ConvLayer(ni, ni, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "                            SimpleSelfAttention(ni, ks=1),\n",
    "                            ConvLayer(ni, ni, act_cls=act_cls, norm_type=norm_type, **kwargs),\n",
    "                            SimpleSelfAttention(ni, ks=1))\n",
    "    else: raise NotImplementedError(f'Bottleneck architecture {bottleneck} not implemented.')\n",
    "    apply_init(seq, init)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(self, encoder, bottleneck=None, blur=False, blur_final=True,\n",
    "                 norm_type=None, act_cls=defaults.activation, init=nn.init.kaiming_normal_, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_chs = encoder.feature_info.channels()[::-1]\n",
    "        encoder_reds = encoder.feature_info.reduction()\n",
    "        skip_channels = L(encoder_chs[1:])\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        up_c = encoder_chs[0]\n",
    "        self.bottleneck = _make_bottleneck(bottleneck, up_c, act_cls=act_cls, norm_type=norm_type, init=init, **kwargs) if isinstance(bottleneck,str) else bottleneck\n",
    "        for i,skip_c in enumerate(skip_channels):\n",
    "            not_final = i!=len(skip_channels)-1\n",
    "            do_blur = blur and (not_final or blur_final)\n",
    "            scale = encoder_reds.pop()//encoder_reds[-1]\n",
    "            block = DecoderBlock(up_c, skip_c, scale=scale, blur=do_blur, final_div=not_final, act_cls=act_cls, init=init, norm_type=norm_type, **kwargs)\n",
    "            up_c = block.nf\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        scale = encoder_reds[0]\n",
    "        self.final_shuf = PixelShuffle_ICNR(up_c, scale=scale, act_cls=act_cls, norm_type=norm_type, **kwargs) if scale!= 1 else None\n",
    "        self.nf = up_c\n",
    "\n",
    "    def forward(self, x: List[torch.Tensor]):\n",
    "        x.reverse()  # torchscript doesn't work with [::-1]\n",
    "        skips = x[1:]\n",
    "        x = x[0]\n",
    "        if self.bottleneck is not None: x = self.bottleneck(x)\n",
    "        for i, b in enumerate(self.blocks):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = b(x, skip)\n",
    "        if self.final_shuf is not None: x = self.final_shuf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unet also needs its head, the final sequence of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class UnetHead(SequentialEx):\n",
    "    def __init__(self, up_c, n_out, last_cross=False, n_in=None, bottle=False, norm_type=None, act_cls=defaults.activation, init=nn.init.kaiming_normal_, y_range=None, **kwargs):\n",
    "        layers = nn.ModuleList([ResizeToOrig()])\n",
    "        if last_cross:\n",
    "            if n_in is None: raise AttributeError('You must specify `n_in` if `last_cross=True`.')\n",
    "            up_c += n_in\n",
    "            layers.extend([MergeLayer(dense=True), ResBlock(1, up_c, up_c//2 if bottle else up_c, act_cls=act_cls, norm_type=norm_type, **kwargs)])\n",
    "        layers.append(nn.Conv2d(up_c, n_out, kernel_size=(1, 1)))\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        super().__init__(*layers)\n",
    "        apply_init(self, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine them all together into a Unet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class TimmUnet(SequentialEx):\n",
    "    \"\"\"Unet is a fully convolution neural network for image semantic segmentation\n",
    "    Args:\n",
    "        encoder: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        n_in: number of input channels.\n",
    "        n_out: number of output_channels\n",
    "        encoder_kwargs: kwargs for the encoder\n",
    "        encoder_indices: indices of layers at which encoder features are extracted.\n",
    "        blur: use blur inside PixelShuffle_ICNR.\n",
    "        blur_final: use blur inside the final decoder block.\n",
    "        bottleneck: one of 'conv' (two conv blocks), 'attention' (conv block followed by SimpleSelfAttention),\n",
    "            'double_attention' (like 'attention' but twice in series).\n",
    "        last_cross: concatenate input before final layers.\n",
    "        pretrained: use pretrained weights in the encoder.\n",
    "        y_range: attach a scaled sigmoid layer to the end.\n",
    "        bottle: bottleneck structure in the final ResBlock layer (if last_cross).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder='resnet50', n_in=3, n_out=1, encoder_kwargs=None, encoder_indices=None,\n",
    "                 blur=False, blur_final=True,\n",
    "                 bottleneck=None, last_cross=True, norm_type=None, act_cls=defaults.activation,\n",
    "                 init=nn.init.kaiming_normal_, pretrained=True, y_range=None, bottle=False, **kwargs):\n",
    "        encoder_kwargs = encoder_kwargs or {}\n",
    "        # NOTE some models need different backbone indices specified based on the alignment of features\n",
    "        # and some models won't have a full enough range of feature strides to work properly.\n",
    "        self.encoder = create_model(encoder, features_only=True, out_indices=encoder_indices, in_chans=n_in,\n",
    "                                    pretrained=pretrained, **encoder_kwargs)\n",
    "\n",
    "        self.decoder = UnetDecoder(self.encoder, bottleneck=bottleneck, blur=blur, blur_final=blur_final,\n",
    "                                   norm_type=norm_type, act_cls=act_cls, init=init, **kwargs)\n",
    "\n",
    "        self.head = UnetHead(self.decoder.nf, n_out, last_cross=last_cross, n_in=n_in, bottle=bottle,\n",
    "                             norm_type=norm_type, act_cls=act_cls, init=init, y_range=y_range, **kwargs)\n",
    "        super().__init__(nn.Sequential(self.encoder, self.decoder), *self.head)\n",
    "\n",
    "    @property\n",
    "    def default_cfg(self): return self.encoder.default_cfg\n",
    "    @property\n",
    "    def feature_info(self): return self.encoder.feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to define an appropriate `Learner` factory function. We can make use of one more aspect of `timm`: each model comes pretrained and carries metadata about the mean and standard deviation of the dataset and about the indices of early, middle and late layers. These are all used by `fastai` to facilitate transfer learning and we certainly want to take advantage of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _timm_stats(m): return tuple((list(m.default_cfg[stat]) for stat in ('mean','std')))\n",
    "def _get_params(m, ls): return params(nn.Sequential(*getattrs(m, *ls)))\n",
    "def _timm_splitter(m):\n",
    "    encoder_modules = L(m.encoder._modules)\n",
    "    encoder_split_module = m.feature_info.module_name(0)\n",
    "    encoder_split = encoder_modules.index(encoder_split_module)\n",
    "    encoder_early = _get_params(m.encoder, encoder_modules[:encoder_split])\n",
    "    encoder_late = _get_params(m.encoder, encoder_modules[encoder_split:])\n",
    "    decoder = _get_params(m.decoder, L(m.decoder._modules))\n",
    "    head = _get_params(m.head, L(m.head._modules))\n",
    "    return L(encoder_early, encoder_late, L(*decoder, *head))\n",
    "\n",
    "@delegates(TimmUnet.__init__)\n",
    "def timm_unet_learner(dls, arch, normalize=True, n_in=None, n_out=None, pretrained=True,\n",
    "                 # learner args\n",
    "                 loss_func=None, opt_func=Adam, lr=defaults.lr, splitter=None, cbs=None, metrics=None, path=None,\n",
    "                 model_dir='models', wd=None, wd_bn_bias=False, train_bn=True, moms=(0.95,0.85,0.95),\n",
    "                 # other model args\n",
    "                 bottleneck='conv',\n",
    "                 **kwargs):\n",
    "    \"Build a Unet learner with a timm backbone from `dls` and `arch`\"\n",
    "\n",
    "    n_in = ifnone(n_in, dls.one_batch()[0].shape[1])\n",
    "    n_out = ifnone(n_out, get_c(dls))\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    model = TimmUnet(arch, n_in=n_in, n_out=n_out, pretrained=pretrained, bottleneck=bottleneck, **kwargs)\n",
    "\n",
    "    if normalize: _add_norm(dls, {'stats': _timm_stats(model)}, pretrained)\n",
    "    splitter=ifnone(splitter, _timm_splitter)\n",
    "    learn = Learner(dls=dls, model=model, loss_func=loss_func, opt_func=opt_func, lr=lr, splitter=splitter, cbs=cbs,\n",
    "                   metrics=metrics, path=path, model_dir=model_dir, wd=wd, wd_bn_bias=wd_bn_bias, train_bn=train_bn,\n",
    "                   moms=moms)\n",
    "    if pretrained: learn.freeze()\n",
    "    # keep track of args for loggers\n",
    "    store_attr('arch,normalize,n_out,pretrained', self=learn, **kwargs)\n",
    "    return learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
